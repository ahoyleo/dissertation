This dissertation proposes the framework of semantic data mining, a novel direction for the field of data mining that focuses on systematic incorporation of domain knowledge. The enabling technology is based on two contributions presented in the dissertation. First, we develop a graph-based formalism that allows a coherent representation for both the data and the domain knowledge. The key concepts of the approach are bridging the data and the ontology by semantic annotation and employing the RDF bipartite graph as the unified representation. Second, we demonstrate analysis techniques that can be carried out based on the RDF bipartite graph to tackle common data mining tasks, such as the frequent itemset mining, while at the same time leveraging domain knowledge to enhance the performance. For this purpose, several graph-based similarity measures are provided as the key components in the mining algorithms and their trade-offs are studied. The concept of random walk inside the graph while traversing and calculating similarities among nodes are used in designing these measures. 

This dissertation also presents the details of some case studies that have validated the hypotheses used in designing the graph-based semantic data mining framework.

\section{Future Work}
Semantic data mining is a new field, many interesting research directions related to it are yet to be explored. The research work presented in this dissertation can be extended in several directions. The following are the main directions we have identified.

\subsection{Learning Weights Automatically}
\subsection{Handling Numeric Values}
\subsection{Scalability Issues}
The calculation of eigenvectors of the Laplacian of a large graph is very expensive. Lin and Cohen~\cite{LinEtal2010ICML} proposed an approximation to a eivenvalue-weighted linear comination of all the eigenvectors, which can be achieved by performing a small number of matrix-vector multiplications.  Such procedure results in a simple and scalable method called power iteration clustering that finds a very low-dimensional data embededing using truncated power iteration on a normalized pair-wise similarity matrix of the data points.

Zhao \etal~\cite{ZhaoEtal2011Eff} described the idea of graph coordinate systems, which embeds graph nodes into points on a coordinate system. By allowing lower distance distortion errors, they were able to develop a practical system that provides fast embedding of large graphs in a hyperbolic space. The embedding algorithm can be parallelized to allow the cost of embedding process being spread across multiple servers. Furthermore, they presented a method to use graph coordinates to efficiently locate shortest paths between node pairs. Such concept can be naturally extended to embed graph nodes according to their commute time distance.

Savas and Dhillon~\cite{SavasEtal2011Clu} introduced a novel framework called clustered low rank matrix approximation for massive graphs. By first partitioning the vertices into a set of disjoint clusters with a fast procedure we are able to preserve important structural information of the original graph. Then we compute a low rank approximation of each cluster independently. Finally the different cluster-wise approximations are combined using an optimal projection step to obtain a low rank approximation of the entire graph, thus including connections or edges between vertices from different clusters. Recently, stochastic algorithms have been developed that use randomness to obtain low rank approximations of a given matrix [16, 10, 23]. The resulting approximations are not optimal, but the algorithms are fast and simple to implement. In addition, the approximation errors can be bounded due to their stochastic nature. As a second contribution of this paper, we extend stochastic methods to the clustered low rank approximation framework and prove corresponding theoretical bounds for the approximation errors. As a third contribution, we show that the proposed methods perform very well in practice. 

\subsection{New Ways of Representing Complex Domain Semantics}
In this project, we plan to investigate two basic approaches.
First, in certain applications, we can explicitly describe the desired or acceptable walk (traversal sequence) in the RDF hypergraph. In this case, we plan to leverage the recently proposed regular traversal expression ~\cite{Marko10} and incorporate it in to random walk. In the basic case, we can specify only certain types of nodes in a given random walk. The regular traversal expression can allow us to even specify acceptable path segments or sequences.
However, the fast power-iteration approach for computing the stationary probability may not be applicable any more due to the label sequence constraint.
To address this problem, we can apply the Monte-Carlo simulation of the random walk to approximate the similarity measure. Note that this approach can be rather scalable as the simulation can be in general constrained in those nodes linking two targeted nodes.
In the second approach, we can utilize the regular random walk but assigning different  weights to nodes with different semantic types. In this case, the main challenge is how to scale the existing  Hypergraph Laplacian to very large graphs.

\section{Concluding Remarks}