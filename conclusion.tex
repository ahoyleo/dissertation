This dissertation proposes the framework of semantic data mining, a novel direction for the field of data mining that focuses on systematic incorporation of domain knowledge. The enabling technology is based on two contributions presented in the dissertation. First, we develop a graph-based formalism that allows a coherent representation for both the data and the domain knowledge. The key concepts of the approach are bridging the data and the ontology by semantic annotation and employing the RDF bipartite graph as the unified representation. Second, we demonstrate analysis techniques that can be carried out based on the RDF bipartite graph to tackle common data mining tasks, such as the frequent itemset mining, while at the same time leveraging domain knowledge to enhance the performance. For this purpose, several graph-based similarity measures are provided as the key components in the mining algorithms and their trade-offs are studied. The concept of random walk inside the graph while traversing and calculating similarities among nodes are used in designing these measures.

This dissertation also presents the details of some case studies that have validated the hypotheses used in designing the graph-based semantic data mining framework.

\section{Future Work}
Semantic data mining is an emerging field, and many interesting research directions related to it are yet to be explored. The research work presented in this dissertation can be extended in several directions. The following are the main directions we have identified.

\subsection{Learning Weights Automatically}
The RDF bipartite graph representation relies on the assignment of weights to different types of hyperedges to distinguish the underlying semantics they represent. When ontologies are involved in the mining process, there are at least two types of hyperedges involved, corresponding to RDF statements coming from data and ontologies respectively. Thus at least two different weights have to be assigned with respect to this distinction. In the current work, the (ratio of) weights can be decided purely empirically, or through a series of trial and error experiments on a sampled sub-dataset, which is hardly guaranteed to be accurate or generalizable. Such difficulty is even more noticeable when there are multiple semantic relationships present in the data and ontologies that one hopes to distinguish so as to achieve finer-grained control over their respective contributions to the mining result. Therefore how to automatically derive suitable weights is an important research question.

One technique that can be used is to train a prediction model from labeled data. This approach suffers from the difficulty to acquire the gold-standard training sample. Tian \etal~\cite{Tian2009AHyper} proposed an semi-supervised approach for classifying nodes in a graph based on a relatively small labeled set. The main idea is to formalize the weight assignment and label propagation in one constraint optimization problem while the two objectives can be alternately solved using a two-step iterative method. While this approach is promising, how to extend it to other mining tasks such frequent pattern mining is still an open question.
\subsection{Handling Continuous Features}
The RDF bipartite graph is straightforward to represent binary-valued data and also able to represent nominal-valued data through RDB nominal value expansion. However, there is no immediate solution to make numerical (continuous) data representable. If we enumerate all values present in a numerical feature and create one node in the graph for each of such values, the size of the resulting graph is bound to become intractable. A common way to handle continuous feature is discretization, or binning, as in making histograms. Typical discretization methods include equal interval, equal frequency partitioning, or more sophisticated method such as Fayyad and Irani's supervised method called MDL~\cite{FayyadI93discretization} that uses information gain to recursively define the best bins. The more interesting part comes when discretization has to be guided by domain knowledge. For example, certain patterns make better sense when a column of dates is present and discretized into seasons or quarters rather than arbitrary time intervals. How to represent and execute such domain knowledge in a way that is adaptable to the graph-based semantic data mining framework is a matter of ongoing research. For example the process of domain knowledge guided discretization may hint at a need for a rule-based set of data transformation routines authored under a well-defined protocol that can be treated as a preprocessing step before transforming data to graphs. More ways to handle domain knowledge in a standardized way are discussed later in this section.

\subsection{Scalability Issues}
The presented graph-based semantic mining framework heavily relies on the notion of graph-based similarity. We describe the use of several random walk-based measures. The calculation of eigenvectors of the Laplacian of a large graph to derive the similarity measures is very expensive. Real non-trivial practical problems are often associated with large scales. For instance, there are
more than 30,000 classes in the well-known Gene Ontology [82] and big social networks have hundreds of millions of users. Therefore scalability of the graph-based semantic mining methods are of critical importance.

The general solution to is to employ approximation and develop parallelizable algorithms. Lin and Cohen~\cite{LinEtal2010ICML} proposed an approximation to a eigenvalue-weighted linear combination of all the eigenvectors, which can be achieved by performing a small number of matrix-vector multiplications.  Such procedure results in a simple and scalable method called power iteration clustering that finds a very low-dimensional data embedding using truncated power iteration on a normalized pair-wise similarity matrix of the data points. Zhao \etal~\cite{ZhaoEtal2011Eff} described the idea of graph coordinate systems, which embeds graph nodes into points on a coordinate system. By allowing lower distance distortion errors, they were able to develop a practical system that provides fast embedding of large graphs in a hyperbolic space. The embedding algorithm can be parallelized to allow the cost of embedding process being spread across multiple servers. Furthermore, they presented a method to use graph coordinates to efficiently locate shortest paths between node pairs. Such concept can be naturally extended to embed graph nodes according to their commute time distance. Savas and Dhillon~\cite{SavasEtal2011Clu} introduced a novel framework called clustered low rank matrix approximation for massive graphs. The first step is to partition the vertices into a set of disjoint clusters with some fast procedure to preserve important structural information of the original graph. Then a low rank approximation of each cluster is computed independently. Finally the different cluster-wise approximations are combined using an optimal projection step to obtain a low rank approximation of the entire graph, thus including connections or edges between vertices from different clusters. While all these techniques are promising, we will need to extend and them to the RDF bipartite graph and stratification (between data and ontologies and among different semantic types) of the graph may require further adaptions and modifications to those algorithms.

\subsection{New Ways of Representing Complex Domain Semantics}
In this project, we plan to investigate two basic approaches.
First, in certain applications, we can explicitly describe the desired or acceptable walk (traversal sequence) in the RDF hypergraph. In this case, we plan to leverage the recently proposed regular traversal expression ~\cite{Marko10} and incorporate it in to random walk. In the basic case, we can specify only certain types of nodes in a given random walk. The regular traversal expression can allow us to even specify acceptable path segments or sequences.
However, the fast power-iteration approach for computing the stationary probability may not be applicable any more due to the label sequence constraint.
To address this problem, we can apply the Monte-Carlo simulation of the random walk to approximate the similarity measure. Note that this approach can be rather scalable as the simulation can be in general constrained in those nodes linking two targeted nodes.
In the second approach, we can utilize the regular random walk but assigning different  weights to nodes with different semantic types. In this case, the main challenge is how to scale the existing  Hypergraph Laplacian to very large graphs.

\section{Concluding Remarks}