This dissertation proposes the framework of semantic data mining, a novel direction for the field of data mining with a focus on the incorporation of domain knowledge encoded in ontologies. The enabling technologies are based on three contributions presented in the dissertation.

first, we propose a graph-based formalism that allows a coherent representation for both data and ontologies. The key concept of the approach is to use RDF as a common ground for both data and domain knowledge encoded in ontologies and then to employ the RDF hypergraph or bipartite graph as the unified representation.

Second, when mining tasks require accessing disparate, heterogeneous data sources, we develop a method based on metaheuristic optimization to automatically resolve schema heterogeneities as well as to achieve pattern comparison for meta-analysis.

Finally, we demonstrate analysis techniques that can be carried out based on the RDF hypergraph or bipartite graph to tackle common data mining tasks. We showcase the utility of such techniques on a mining problem called semantically associated itemset discovery, a particular kind of frequent itemset mining focusing on finding indirect connections. For this purpose, several graph-based similarity measures are provided as key components of mining algorithms. Their capacities, limitations and trade-offs are studied. The concept of random walk on hypergraphs while traversing and calculating similarities among nodes are used in designing these measures.

This dissertation also presents the details of some case studies that have validated the hypotheses used in designing the graph-based semantic data mining framework.

\section{Future Work}
Semantic data mining is an emerging field, and many interesting research directions related to it are yet to be explored. The research work presented in this dissertation can be extended in several directions. The following are some of the most important ones we have identified.

\subsection{Automatic and Robust Semantic Annotation}
Semantic annotation is crucial in realizing semantic data mining by bridging formal semantics in Semantic Web meta-data with data. It aims at assigning semantic descriptions to elements of data. %To ease the burden of common users that are not familiar with the Semantic Web, we develop a learning-based semantic search algorithm to suggest appropriate semantic descriptions for annotation.
The annotation process can be generally divided into two steps. The first is to establish mappings between existing Semantic Web terms and terms need to be annotated in the data. The second step is to come up with a local ontological structure constituting the Semantic Web terms to model the data. Most of previous work focus on the second step. Some skip the first step and bootstrap the ontological terms and structure from the local data itself. For example, a number of systems that map data in RDB to RDF leverage a set of rules such as ``table to class and column to predicate." Other examples of rules involved in mapping RDB schema to OWL ontologies include ``foreign keys to object property and non-key attributes to datatype property."
Similar ideas have been adopted in annotating semi-structured data. Existing spreadsheet-to-rdf tools typically map spreadsheets to star-shaped RDF graphs. Some tools try to express richer spreadsheet semantics, e.g., Han et al. developed a spreadsheet-to-rdf tool called RDF123~\cite{RDF123} that allows users to define mappings to arbitrary graphs.

We argue that mapping RDB or spreadsheet to linked data (e.g., RDF) without reference to existing semantic descriptions does not lend itself well to aiding semantic data mining. The automatically constructed self-contained local ontology may be applicable to describe a specific dataset but is most likely too rough to capture the full domain semantics that is necessary to express meaningful domain knowledge. Moreover, with the advent of the Semantic Web and pervasive connectivity, an increasing number of ontologies have been made widely available for reuse. These ontologies are created by thorough knowledge engineering process and should serve as better models for annotation. However, on the other hand, the sheer number of Semantic Web ontologies and lack of effective search functionality can lead to a huge hidden barrier for common users. Choosing proper Semantic Web ontologies and terms (classes and properties) requires familiarity with appropriate ontologies and the terms they define. There is very few system that is able to provide automatic suggestions.

To solve this problem, we proposed a preliminary idea of learning-based semantic search algorithm reported in~\cite{liu2010} to suggest proper Semantic Web terms and ontologies for annotation given semantically related words and general domain and context information. The evaluation of the algorithm is a matter of ongoing research.

%In order to suggest suitable Semantic Web terms and ontologies for users to annotate their data, we propose a learning-based semantic search algorithm. We first submit a list of terms appeared in the schema of (semi-)structured data to our semantic search algorithm and then use the returned results for annotation. In a fully automatic setting, the search algorithm is configured to return the top-1 hit; while in an interactive setting, the search algorithm returns ordered top-k search results for users to decide. Previous semantic search algorithms leverage a variety of measures, including lexical and structural similarities (see details below) to rank Semantic Web documents according to how likely they can be semantically matched to the search terms. However, using any single measure alone may not be sufficient to achieve the optimal result. We propose to combine various measures to a weighted feature-based search model, where the weights are learned from training data. We believe the incorporation of learning techniques will improve the semantic search result. We have built a prototypical system for gathering training data for two algorithms: logistic regression and subgradient descent. The evaluation of such method is a matter of ongoing research.


\subsection{Learning Weights Automatically}
The RDF bipartite graph representation relies on the assignment of weights to different types of hyperedges, or paths in bipartite graphs, to distinguish the underlying semantics they represent. When ontologies are involved in the mining process, there are at least two types of connections involved, corresponding to RDF statements coming from data and ontologies respectively. Thus at least two different weights have to be assigned with respect to this distinction. In the current work, the (ratio of) weights can be decided purely empirically, or through a series of trial and error experiments on a sampled sub-dataset, which is hardly guaranteed to be accurate or generalizable. Such difficulty is even more noticeable when there are multiple semantic relationships present in the data and ontologies that one hopes to distinguish so as to achieve finer-grained control over their respective contributions to the mining result. Therefore how to automatically derive suitable weights is an important research question.

One technique that can be used is to train a prediction model from labeled data. This approach suffers from the difficulty of acquiring the gold-standard training sample. Tian \etal~\cite{Tian09} proposed a semi-supervised approach for classifying nodes in a graph based on a relatively small labeled set. The main idea is to formalize the weight assignment and label propagation in one constraint optimization problem while the two objectives can be alternately solved using a two-step iterative method. While this approach is promising, how to extend it to other mining tasks such as frequent pattern mining is worth of further investigation.

\subsection{Handling Continuous Features}
The RDF bipartite graph is straightforward to represent binary-valued data and is also able to represent nominal-valued data through RDB nominal value expansion. However, there is no immediate solution to make numerical (continuous) data representable. If we enumerate all values present in a numerical feature and create one node in the graph for each of such values, the size of the resulting graph is bound to become intractable. A common way to handle continuous feature is discretization, or binning, as in making histograms. Typical discretization methods include equal interval/frequency partitioning, or more sophisticated ones such as Fayyad and Irani's supervised method called MDL~\cite{FayyadI93discretization} that uses information gain to recursively define the best bins.

The more interesting part comes when discretization has to be guided by domain knowledge. For example, certain patterns may make better sense when a column of dates is present and discretized into seasons or quarters rather than arbitrary time intervals. How to represent and execute such domain knowledge in a way that is adaptable to the graph-based semantic data mining framework is a matter of ongoing research. For example the process of domain knowledge guided discretization may hint at a need for a set of rule-based data transformation routines under a well-defined protocol that can be treated as a preprocessing step before converting data to graphs. More ways to handle domain knowledge in a standardized way are discussed later in this section.

\subsection{Scalability Issues}
The presented graph-based semantic mining framework heavily relies on the notion of graph-based similarity. We describe the use of several random walk-based measures. The matrix operations required to derive the similarity measures and is very expensive on large graphs. Non-trivial practical problems are often associated with large scales. For instance, there are more than 30,000 classes in the well-known Gene Ontology, and big online social networks have hundreds of millions of users. Therefore scalability of the graph-based semantic mining methods are of critical importance.

The general solution is to employ approximation and develop parallelizable algorithms. Lin and Cohen~\cite{LinEtal2010ICML} proposed an approximation to an eigenvalue-weighted linear combination of all the eigenvectors, which can be achieved by performing a small number of matrix-vector multiplications.  Such procedure results in a simple and scalable method, called power iteration clustering, that finds a very low-dimensional data embedding using truncated power iteration on a normalized pair-wise similarity matrix of the data points. Zhao \etal~\cite{ZhaoEtal2011Eff} described the idea of embedding graph nodes into points on a coordinate system. By allowing lower distance distortion errors, they were able to develop a practical system that provides fast embedding of large graphs in a hyperbolic space. The embedding algorithm can be parallelized to allow the cost of the embedding process to be spread across multiple servers. Furthermore, they presented a method to use graph coordinates to efficiently locate shortest paths between node pairs. Such a concept can be naturally extended to embed graph nodes according to their commute time distance. Savas and Dhillon~\cite{SavasEtal2011Clu} introduced a novel framework called clustered low rank matrix approximation for massive graphs. The first step is to partition the vertices into a set of disjoint clusters with some fast procedure to preserve important structural information of the original graph. Then a low rank approximation of each cluster is computed independently. Finally the different cluster-wise approximations are combined using an optimal projection step to obtain a low rank approximation of the entire graph, thus including connections or edges between vertices from different clusters. While all these techniques are promising, we will need to extend them to the RDF bipartite graph, and the stratification (between data and ontologies and among different semantic types) of the graph may require further adaptions and modifications to those algorithms.

\subsection{New Ways of Representing Complex Domain Semantics}
The RDF bipartite graph can represent concrete semantics such as the ``\emph{is\_a}" or ``\emph{located\_in}" relationship. However, meta semantics such as domain/range and cardinality constraints are not so straightforward to be modeled.

One possible approach that can be used to enhance the ability of handling more complex domain semantics in certain applications is to model domain constraints by explicitly describing the desired or acceptable walk (traversal sequence) in the RDF hypergraph or bipartite graph. In this case, the recently proposed regular traversal expression ~\cite{Marko10} is worth investigation. In the basic case, we can specify only certain types of nodes in a given random walk. The regular traversal expression can allow us to even specify acceptable path segments or sequences. However, the fast power-iteration approach for computing the stationary probability may not be applicable any more due to the label sequence constraint. To address this problem, we can apply the Monte-Carlo simulation of the random walk to approximate the similarity measure. Notice that this approach can be rather scalable as the simulation can be in general constrained in those nodes linking two targeted nodes.

\section{Concluding Remarks}
Although it is widely acknowledged that the use of domain knowledge is important in all stages of the data mining process, research on systematic fusion of the knowledge and data mining still remains in its early stages. The conventional way of using domain knowledge often causes a tight coupling of assumptions and algorithms, and hence may hinder the maintainability and interoperability of mining systems. We propose a new angle to attack this problem by introducing a graph-based formalism. Domain knowledge is encoded in ontologies which are in turn represented as RDF hypergraphs, the same representation we can use to model data. In this way, domain knowledge is treated as first-class objects in the mining algorithm, and the central task becomes first finding good quality ontologies that captures domain semantics and then determining a relevant subset of the ontologies that should be part of the mining process. The relevant strength of the relationships in ontologies can be defined by assigning proper weights to them. Several graph-based similarity measures are provided as key components of mining algorithms on the unified graph representation. Their capacities, limitations, trade-offs, and possible directions for improvement are studied on a series of synthetic and real-world case studies.

We believe that the designed principles for semantic data mining with graph-based approaches provide a novel and useful way to incorporate domain knowledge. As such they should be considered the most important contributions of this dissertation. 