This chapter covers the background areas and related work necessary to understand the contributions of this dissertation. It discusses the current state of the art efforts to incorporate domain knowledge in data mining. In addition, it describes the use of graphs in data mining with a focus on graph-based similarities. Researches in metaheuristics optimization, schema matching, cluster comparison and so forth are also briefly discussed.

\section{Domain Knowledge in Data Mining}
Domain knowledge relates to information about a specific domain or data that is collected from previous systems or documentation, or elicited from domain experts. In the rest of the section, we highlight a body of studies that aims at exploring ways to employ domain knowledge in data mining. The results from these studies strongly attest to the positive influence of domain knowledge. Domain knowledge can affect the discovery process within a data mining system in at least two ways. First, it can make patterns more visible by generalizing the attribute values, and second, it can constrain the search space as well as the rule space.

In order to effectively summarize and compare different previously proposed systems, we propose a reference framework to classify different kinds of domain knowledge at a very high abstraction level as detailed in the following.

\begin{itemize}
\item	Knowledge about the domain: This category contains information related to a specific domain of discourse, usually obtained from either domain experts or previous data mining processes. Examples of such knowledge include concept hierarchy, integrity constraints, \etc.
\item	Knowledge about the data: This category contains information about a dataset, including how it is generated, transformed and evolved. This knowledge is obtained from data producers (people who carry out experiments or collect data) or database managers. For example, in a database of spatial information, one of the images may have been recorded with a very skew angle on the object. When processing the database the discovery process must take this information into account.
\item	Knowledge about the data mining process: This category contains information pertaining to specific data mining tasks, including goals, parameters and variables related to the experiment. For example, attributes of interest within data, and the measure of interestingness for discovered patterns.
\end{itemize}

The summarized work can be divided into two groups. The first group does not explicitly leverage any knowledge representation approaches to model domain knowledge. The second group explores mainly ontological knowledge (concept hierarchy) and uses formal ontology languages to encode such knowledge. The kind of domain knowledge involved in the first group is broader which covers all categories discussed in the above reference classification scheme. However, it is achieved at the cost of less formality which often results in ad-hoc expression of domain knowledge that has a very application-specific form, scop and granularity.

%Various studies have shown that employing domain knowledge can benefit data mining in a number of ways. Sinha et al. [18] postulated that infusing domain knowledge would reduce the hypothesis space, thereby facilitating search for a solution. Yoon et al. [26] described a similar idea and provided several illustrative motivating examples. They demonstrated that domain knowledge can be used to reduce the size of the database by eliminating data records or irrelevant attributes that are not needed for discovery or by refining the original query via posing more restrictions. In other words, with domain knowledge, data mining queries can be transformed into forms that are more efficiently processable.

%Daniels et al. observed difficulties when applying models derived from data mining processes without considering domain knowledge [11]: i) Incompatibility of mined model and domain knowledge, ii) lack of interpretability of the model. Quite often it is more important to gain insight in the decision problem than to have accurate predictions. And iii) knowledge representation at the wrong level of detail. Data mining algorithms often yield structures or models that are intractable for human decision makers due to their huge complexity. They further claimed that, in a variety of business intelligence applications, domain knowledge would improve the efficiency of data mining systems: i) Risk assessment in the presence of both qualitative knowledge and legal or contractual constraints. ii) Classification and description of customer groups in evaluation decision processes such as credit loan evaluation and fraud detection. iii) Validation of business rules especially in distributed user environments. And iv) all kinds of price models for trend analysis or automatic trading employed in combination with transaction databases.

%Despite that the benefits abound, it is known that there still has been little research focusing on the combination of domain knowledge and data to date. Sinha et al. [18] elaborated that the reason of the lack of attention to the role of domain knowledge in data mining research partly attribute to the fact that domains of application are typically data-rich, giving the impression that domain knowledge is not really necessary. Also, the fact that in many domains expert knowledge is not readily available or, even when available, knowledge acquisition tends to be a difficult and time-consuming process, could be a factor in not addressing the issue of domain knowledge.

%We end this section by providing a summary of the ontology-based data mining systems discussed throughout this section. Table 1 shows how they are classified on the dimensions we have discussed.

In one of the earliest studies on the subject, Pazzani and Kibler~\cite{Pazzani1992} developed a general purpose relational learning algorithm called FOCL, which combines explanation-based and inductive learning. In a later study, they conducted an experiment comparing FOCL with a domain theory to FOCL without a domain theory. A partial knowledge base of an expert system was used as the domain theory. They found incorporating domain theory significantly reduced misclassification costs when larger training sets were used.

In another study, Ambrosino and Buchanan~\cite{Ambrosino1999} examined the effects of adding domain knowledge to a rule induction program for predicting the risk of mortality in patients with community--acquired pneumonia. They developed a graphical data exploration tool for domain experts to encode domain knowledge and interact with the data mining process. The domain experts participated in two stages of mining. They were first asked to modify the existing set of attributes according to their domain knowledge, and then they were prompted with mining results and were able to modify the mined models (rules) directly. The experiment contained an experimental where the domain knowledge was incorporated as mentioned above, and a control group without domain knowledge. The experimental group performed significantly better (lower percent mean error) than the control group.

Sinha and Zhao~\cite{Sinha08} conducted an extensive comparative study on the performance of seven data mining classification methods---naive Bayes, logistic regression, decision tree, decision table, neural network, k-nearest neighbor, and support vector machine---with and without incorporating domain knowledge. The application they focused on was in the domain of indirect bank lending. An expert system capturing a lending expert's knowledge of rating a borrower's credit is used in combination with data mining to study if the incorporation of domain knowledge improves classification performance. In their study, the domain knowledge used was partial, meaning that it could only lead to intermediate results but was not sufficient to make the final prediction. They cascaded the expert system with a data mining classifier. The experiment adopted an experimental vs. control paradigm, similar to Ambrosino et al.'s early experiment in 1999. The prediction proposed by the expert system was added to other inputs. Classifiers built using input data enhanced by the expert system's output formed the experimental group. For the control group, classifiers were built using the original set of input attributes (bypassing the expert system). Their results showed that incorporation of domain knowledge significantly improves classification results with respect to both misclassification cost and AUC (Area Under Curve). Hence they concluded by calling for more attention in combining domain knowledge and data mining. They articulated that, in knowledge engineering, the focus is on the knowledge of a human expert in a specific problem area. On the other hand, the focus of data mining is on the data available in an organization. Expert systems and data mining methods could play complementary roles in situations where both knowledge and data are available.

Hirsh and Noordewier~\cite{Hirsh94} argued that if learning is to be successful, the training data must be encoded in a form that lets the learner recognize underlying regularities. The application domain they focused on was the problem of DNA sequence classification. They proposed to use background knowledge of molecular biology to re-express data in terms of higher-level features that molecular biologists use when discussing nucleic-acid sequences. The high level features were Boolean valued, representing the presence or absence of the feature in a given DNA sequence. Using C4.5 decision trees and backprop neural networks, they conducted experiments with and without the higher-level features. For both learning methods, the use of higher-level features resulted in significantly lower error rates.

Pohle~\cite{Pohle03} contended that data mining techniques are good at generating useful statistics and finding patterns in large volumes of data, but ``they are not very smart in interpreting these results, which is crucial for turning them into interesting, understandable and actionable knowledge." The author viewed the lack of sophisticated tool to support incorporating human domain knowledge into the mining process as being the main factor responsible for the limitation. They also pointed out that ontologies were valuable technologies to incorporate domain knowledge and thus they propose to exploit ontologies when integrating knowledge mined from knowledge discovery process to an existing knowledge base.

Kopanas \etal~\cite{Kopanas02} conducted large scale data mining experiment exploring the role of domain knowledge in different phases of a large-scale data mining project, using a case study of customer insolvency in the telecommunication industry. They argued against the claim that data mining approaches eventually will automate the process and lead to discovery of knowledge from data with little or no support of domain experts and domain knowledge. For each stage in data mining they identified types of domain knowledge involved to either improve the performance or, in some case, make data mining process possible at all. They found that though domain knowledge plays a critical role mainly in the initial and final phases of the project, it influences the other phases to some degree as well. For example, in the problem definition stage, domain knowledge involves business and domain requirements and other implicit, tacit knowledge. In the data preparation stage, the useful domain knowledge involves semantics of corporate database. In the data preprocessing and transformation stage, domain knowledge includes tacit and implicit knowledge for inferences. In feature and algorithm selection stage, main type of knowledge involves how to interpret selected features. In mining stage, domain knowledge focuses on inspection of discovered knowledge. In the evaluation stage, domain knowledge defines performance criteria related to business objectives. In the fielding knowledge base stage (incorporating mined knowledge with an existing knowledge base), domain knowledge provides supplementary information for implementing the fusion.

In another study, Weiss \etal~\cite{Weiss01} combined an expert system with a data mining method for generating better sales leads. They developed an expert system that interviews executives of small and medium-sized companies and, based on their responses, recommends promising sales leads. The question-answer pairs and the recommended solutions were stored as examples to be mined by the method of rule induction. The study demonstrated how a knowledge base can be used to guide a machine learning program. The techniques developed in the study would be useful for consultation systems whose questions have different costs of acquisition.

Daniels \etal~\cite{Daniels01} demonstrated that data mining systems can be successfully combined with explicit domain knowledge. They pointed out that in theory there are two extreme situations that may occur with respect to the availability of domain knowledge. The first is when no prior knowledge that is available. The second is when all relationships are known with certainty, up to a limited number of parameters. They then claimed that their study was positioned somewhere between these extremes. The authors focused on a special type of a priori knowledge, monotonicity, i.e., the sign of relationship between the dependent and independent variables, for economic decision problems. Prior knowledge was implemented as monotonicity constraints in the decision tree and neural network classifiers. Addition of the knowledge resulted in smaller decision trees, and smaller variations of error on the training and test sets for neural networks. The authors also claimed that the framework developed might serve as a tool to implement normative requirements. However, since monotonicity constraints were incorporated in the decision tree and neural networks by designing specific algorithms, it is not obvious how to generalize the algorithm design process to include other normative domain knowledge.

Yoon \etal~\cite{Yoon1999} studied semantic query optimization, a field that endeavors to optimize data mining queries by taking advantage of domain knowledge. The authors demonstrated that significant cost reduction can be achieved by reformulating a query into a less expensive yet equivalent query that produces the same answer as the original one. They identified that in most cases, exhaustive analysis of data is infeasible. It is often necessary to perform a relatively constrained search on a specific subset of data for desired knowledge. The domain knowledge they utilized was classified into three categories, interfiled, category, and correlation, all of which can be represented in rule forms. When a data mining query is received, they first identify domain knowledge that is relevant to the query, and transform it accordingly. On the other hand, to select relevant domain knowledge without an exhaustive search of all domain knowledge, they developed a method that built tables for domain knowledge indexed by attributes.

Vikram and Nagpal~\cite{Vikram2010} developed an iterative association rule mining algorithm to integrate user's domain knowledge with association rule mining. The knowledge they request from the users is attributes of interest. According to users' specification, database is scrutinized to produce a working subset that only contains the attributes of interest while the rest are excluded. With this dataset, the Apriori procedure searches for frequent large itemsets. The advantage is apparent since irrelevant records are filtered out, the result is more meaningful and the running time is also reduced.

We summarize the above surveyed research systems in Table~\ref{tbl:sum_dk_in_dm}. Each system is characterized by 1) its domain of application, 2) type of domain knowledge employed, 3) usage of domain knowledge, and 4) data mining techniques that are adapted to incorporate the domain knowledge.

\begin{landscape}
\begin{table}
\begin{center}
\begin{tabular}{ | p{2.5cm} | p{3cm} | p{4.2cm} | p{4.8cm} | p{3cm} |}
  \hline
    \textbf{System}  &   \textbf{Problem domain}  &   \textbf{Type of domain knowledge}    &   \textbf{Usage of domain knowledge}   &   \textbf{Data mining method} \\
  \hline
    Daniels \etal~\cite{Daniels01}	&  Business Intelligence	& Monotonicity constraints	&  modify mining algorithms to embody the knowledge directly	& Decision tree and neural network  \\
  \hline
    Ambrosino \etal~\cite{Ambrosino1999}	&	Medical decision	&	Attribute-relation, interpretation of result	&	 Experts interact directly with mining in both pre-- and post-- processing stages	&	Decision tree\\
  \hline
    Pazzani \etal~\cite{Pazzani1992}	&	Predicate learning	&	Taxonomy, attribute-relation rules,  attribute correlations	&	 Preprocessing data	&	FOCL\\
  \hline
    Sinha \etal~\cite{Sinha08}	&	Business Intelligence	&	Expert rules	&	Rule's prediction cascaded as an input to classifier	 &	 Seven typical classification algorithms\\
  \hline
    Yoon \etal~\cite{Yoon1999}	&	Query optimization	&	Taxonomy, attribute relation rules and correlation	&	 Transform data mining queries	&	Not specified\\
  \hline
    Hirsh \etal~\cite{Hirsh94}	&	DNA sequence classification	&	Attribute relation rules	&	Forming new set of attributes	 &	 C4.5 and neural network\\
  \hline
    Vikram \etal~\cite{Vikram2010}	&	Association rule mining	&	Attribute of interest	&	Preprocessing data	&	 Association rules\\
  \hline
    Weiss \etal~\cite{Weiss01}	&	Consultation	&	Question-answer pairs derived from interviewing experts	&	 Question-answer pairs serve as part of the input to a mining system	&	No restriction\\
  \hline
    Kopanas \etal~\cite{Kopanas02}	&	Business intelligence	&	Comprehensive information pertaining to a domain	&	 For each stage of mining, discussing the use of certain type of domain knowledge in general	&	No restriction\\
  \hline
\end{tabular}
\end{center}
\caption{\label{tbl:sum_dk_in_dm} Summary of systems that employ domain knowledge.}
\end{table}
\end{landscape}

Next, we describe another line of research on using domain knowledge encoded in ontologies.

Staab and Hotho~\cite{StaabH03} presented an ontology-based text clustering approach. They developed a preprocessing method, called COSA, one of the earliest to utilize the idea of mapping terms in the text to concepts in the ontology. The authors pointed out that the size of the high-dimensional term vector representation of the text document is the principal problem faced by previous algorithms. By mapping terms to concepts, it essentially aggregates terms and reduces the dimensionality.

The mapping of terms to concepts can be also seen as a process of semantic annotation. It was realized in COSA by using some shallow and efficient natural language processing tools. After the mapping process, COSA further reduced the dimensionality by aggregating concepts using the concept heterarchy defined in the ``core ontology" used in their framework. The concept heterarchy should be thought of as equivalent to the subsumption hierarchy (taxonomy) in OWL. The idea was navigating the hierarchy top-down splitting the concepts with most support (number of mapping terms) into their sub-concepts and abandoning the concepts with least support. The rationale was that too frequent or infrequent concept occurrences are not appropriate for clustering. Notice that the definition of a ``core ontology" in the paper was developed prior to the emergence of OWL. COSA pioneers in incorporating ontology in text clustering and displays some generality over the confines of any specific domain.

Wen \etal~\cite{Wen2007Ont} devised a framework that solved the genomic information retrieval problem by using ontology-based text clustering. The core idea was an extension to COSA. Documents containing genomic information were first annotated based on UMLS so that the terms are mapped to concepts. Then the authors pointed out that even the dimension of clustering space is dramatically reduced, there still exists the problem that a document is often full of class-independent ``general" words and short of class-specific ``core" words, which leads to the difficulty of document clustering because class-independent words are considered as noise in clustering. To solve this problem, the authors proposed a technique for concept frequency re-weighing which took into consideration the concept subsumption hierarchy defined in the domain ontology. Finally, from the re-weighed concept vector representation, a cluster language model could be generated for information retrieval.

Fang \etal~\cite{Fang2007Ont} proposed an ontology-based web documents classification and ranking method. The contribution of this work was the introduction of a way to automatically augment or tailor the existing ontology to fit the specific purpose, while in previous work one had to either manually create an ontology from scratch or adopt some well established domain ontology. Their technique was to enrich a certain ontology using terms observed in the text document. This was done with the help of WordNet~\cite{Miller1995WordNet}. Specifically, for example, if the sense of a term appears to be a synonym of the sense of a concept according to WordNet, the term is added to the ontology as a sibling of the concept. The enriched ontology is then treated as a representation of the category to which some text document is classified. The proposed classification was done by simply comparing the similarity between ontologies and the term vector representing the text document. This implied that first, multiple ontologies should be provided for choice, and second, for each category of the corpus there should be one corresponding ontology. These assumptions appeared cumbersome though the authors pointed to the Open Directory Project as a source for initial ontologies in their experiment. Moreover, this process did not fit into traditional classification as there was no training phase. It was more similar to clustering with known number of clusters.

Cheng \etal~\cite{ChengPK03} studied document clustering problem as a means to efficient knowledge management. They utilized ontologies to overcome the ambiguity problem frequently seen in natural language since ``an ontology includes a selection of specific sets of vocabulary for domain knowledge model construction, and the context of each vocabulary is represented and constrained by the ontology." They developed a system called Ontology-based Semantic Classification (OSC) Framework that consisted of two main components: Content-based Free Text Interpreter (CFTI) and Context-based Categorization Agent (CCA). CFTI leveraged on the link grammar capability for syntactical analysis of a sentence. At the same time, the lexical meaning analysis of a sentence was supported through the integration with ontological models such as the WordNet. The context models produced from CFTI correlated the content of a particular document with the context of the user. The role of the CCA was to further enhance the usability of these context models by classifying them according to the user interest. The OSC framework seemed appealing but the authors did not provide any implementation details nor experiment results. It was more of a research proposal and it would be interesting to see the performance of the system when the authors make the proposal a reality.

Taghva \etal~\cite{Taghva2003Ont} reported on the construction of an ontology that applied rules for identification of features to be used for an email classification system, called ``Ecdysis." The ontology was designed for the purpose of encoding expert rules deciding the email category. Therefore it contained only those concepts that were aspects of such rules. CLIPS was used to implement rules and the inference with rules was based on a ``match-and-fire" mechanism: One or more features of an email instance would be matched with instances of classes from the ontology. If there was a successful match, then the rule would fire, causing the email to have some certain feature. This feature became one of many that could be used for training and classification with a Bayesian classifier. The authors claimed that preliminary tests showed that these additional features enhanced the accuracy of the classification system dramatically.

Tenenboim \etal~\cite{Tenenboim2008} proposed an automatic method for classification of news using hierarchical news ontology. The system they developed was called ``ePaper." It was designed to aggregate news items from various news providers and deliver to each subscribed user a personalized electronic newspaper, utilizing content-based and collaborative filtering methods. The ePaper could also provide users ``standard" (\ie, non-personalized) editions of selected newspapers, as well as browsing capabilities in the repository of news items. The classification task performed in the ePaper system aimed at classifying each incoming news document to one or several concepts in the news ontology. In this sense, only the target classes in the classification process were annotated by ontological terms. Since the users' profiles were also defined using the same set of ontological terms, a content-based filter was able to compare the similarity between a user's profile and classified categories of news. Based on results of the classifier and content-based filter, the personalization engine of the system was able to provide a personalized paper.

Lula \etal~\cite{Lula2008} proposed an ontology-based cluster analysis framework. They discussed various aspects of similarity measure between objects and sets objects in an ontology-based environment.  They devised an ontology-based aggregation function to calculate similarity between two objects which takes into account taxonomy similarity, relationship similarity and attribute similarity. For example, path distance, Jaccard coefficient and measures based on information theory can be used to calculate the taxonomy similarity. Relationship similarity can be determined by calculating similarity of objects that participate in the relationship. Attribute similarity can be determined by comparing values of the attributes. The authors claimed that the framework with ontology-based similarity measure opened the possibility for various clustering application. But apparently much work still remained. It was unclear how the aggregation function was defined though each of its components could be solved separately. A proper aggregation was highly possible to be application-specific, which might suggest the need of a learning framework to derive such function.

Li \etal~\cite{Li2005Ont} developed a new decentralized P2P architecture-ontology-based community overlays. The system exploited the semantic property of the content in the network to cluster nodes sharing similar interest together to improve the query and searching performance. To do that, they proposed a query routing approach that organized nodes into community overlays according to different categories defined in the nodes' content ontology. A community overlay was composed of nodes with similar interest. Queries were only forwarded to semantically related overlays, thus alleviating the traffic load. According to taxonomic information in the ontology, peers (nodes) could be clustered into ontological terms. This study introduced a new data mining application besides text document clustering. But their principle remained the same as other related work: ontology is used as an abstraction to data. By incorporating ontologies, some performance metrics of the data mining task can be improved.

Adryan \etal~\cite{Adryan2004} developed a system called GO-Cluster which used the tree structure of the Gene Ontology database as a framework for numerical clustering, and thus allowing a simple visualization of gene expression data at various levels of the ontology tree. Shen \etal~\cite{Shen2006Ont} proposed a new method of association rules retrieval that was based on ontology and Semantic Web. They argued that ontology-based association rules retrieval method can better deal with the problems of rule semantics sharing, rule semantics consistency and intelligibility.

\begin{landscape}
\begin{table}
\begin{center}
\begin{tabular}{ | p{2.5cm} | p{4cm} | p{2.5cm} | p{2.5cm} | p{5cm} |}
\hline
\textbf{System}	&	\textbf{Ontology construction}	&	\textbf{Annotation method}	&	\textbf{Type of sources}	&	 \textbf{Data mining method}\\
\hline
Staab \etal (COSA)~\cite{StaabH03}	&	Manual creation	&	Shallow NLP method	&	Text	&	Clustering based on ``bag-of-concept" representation plus concept aggregation\\
\hline
Wen \etal~\cite{Wen2007Ont}	&	Off-the-shelf (UMLS)	&	Manual	&	Text	&	Clustering based on ``bag-of-concept" representation plus concept frequency reweighing\\
\hline
Fang \etal~\cite{Fang2007Ont}	&	Manual creation of ``core" ontology and update on the fly	&	Manual	&	Text	&	 Clustering based on ``bag-of-concept" representation plus feed back to enrich ontology\\
\hline
Cheng \etal(OSC)~\cite{ChengPK03}	&	Off-the-shelf (WordNet)	&	Rule-based NLP	&	Text	&	Not specified\\
\hline
Taghva \etal (Ecdysis)~\cite{Taghva2003Ont}	&	Manually creation, incorporated with a rule inference system	&	Manual	 &	 Email / text	 &	 Classification with additional features derived from rules\\
\hline
Tenenboim \etal~\cite{Tenenboim2008}	&	Manual creation	&	Manual	&	News archive /text	&	Not specified\\
\hline
Lula \etal~\cite{Lula2008}	&	Not specified	&	Manual	&	Text 	&	Hierarchical agglomerative clustering\\
\hline
Li \etal~\cite{Li2005Ont}	&	Off-the-shelf (Open Directory Project)	&	 Manual	&	P2P user / resource profile data	 &	 Not specified\\
\hline
Adryan \etal~\cite{Adryan2004}	&	Off-the-shelf (Gene Ontology)	&	Manual	&	Gene expressions	&	Hierarchical clustering with instance regrouping based on GO annotation\\
\hline
\end{tabular}
\end{center}
\caption{\label{tbl:sum_dk_as_onto} Summary of ontology-based data mining systems.}
\end{table}
\end{landscape}

In Table~\ref{tbl:sum_dk_as_onto}, we summarize the surveyed data mining systems that make use of ontologies. The table indicates how the solution space is covered by different systems. It shows a large fraction of systems are in the domain of text mining. Most of them make use of taxonomic information provided by ontologies. Only two systems consider incorporating rules. Most systems adopt readily available domain ontologies, while Fang et al's approach can create ontologies on the fly. We also notice that all systems are limited in that they only deal with unstructured input. The importance of automated semantic annotation is generally overlooked in most work.

\section{Graph-based Approach for Knowledge Representation}
Graph-based approaches for representing knowledge have long been used in philosophy, psychology, and linguistics. The computer counterpart to this means is the so-called \emph{semantic network} that represents knowledge in patterns of interconnected nodes and arcs which were first developed for artificial intelligence and machine translation.

The semantic network, and graph-based approaches for knowledge representation in general, are motivated by the desirable qualities of graph for both modeling and computation. From a modeling viewpoint, basic graphs are easily understandable by users, and it is always possible to split up a large graph into smaller ones while keeping its semantics. From the computational viewpoint, the graph is one of the most studied objects in mathematics. Considering graphs instead of logical formulas provides another view of knowledge constructs (\eg, some notions like path, cycle, or connected components are natural on graphs) and provides insights to algorithmic ideas~\cite{CheinMugnier08}. In light of these motivations, what is common to all semantic networks is a declarative graphic representation that can be used either to represent knowledge or to support automated systems for reasoning about knowledge.

According to Sowa~\cite{Sowa91principlesof}, the following are six of the most common kinds of semantic networks.
\begin{enumerate}
\item Definitional networks focus on the is-a or subtype relation among concepts. The resulting network, also called a generalization or subsumption hierarchy, supports the rule of inheritance to propagate properties from a supertype to all of its subtypes. The information in these networks is often assumed to be necessarily true.

\item Assertional networks are designed to assert propositions. Unlike definitional networks, the information in an assertional network is assumed to be contingently true, unless it is explicitly marked with a modal operator. Some assertional networks have been proposed as models of the conceptual structures underlying natural language semantics.

\item Implicational networks use implication as the primary relation for connecting nodes. They may be used to represent patterns of beliefs, causality, or inferences.

\item Executable networks include some mechanism, such as marker passing or attached procedures, which can perform inferences, pass messages, or search for patterns and associations.

\item Learning networks build or extend their representations by acquiring knowledge from examples. The new knowledge may change the old network by adding and deleting nodes and arcs or by modifying numerical values, called weights, associated with the nodes and arcs.

\item Hybrid networks combine two or more of the previous techniques, either in a single network or in separate, but closely interacting networks.
\end{enumerate}

Knowledge such as subsumption hierarchy is best captured by definitional networks. Distance (similarity) measures can usually be reasonably defined on such network, which is essential in many data mining tasks. It is possible to extend data mining algorithms that depend on analyzing distances between entities in factual knowledge to work with distances between those in ontological knowledge.

In addition, one of the most prominent knowledge representation formalism families among current systems, description logics, formerly called terminological logics or concept languages, have been a successful attempt to combine well-defined logical semantics with efficient reasoning~\cite{Sowa91principlesof}. They are derived from an approach proposed by Woods~\cite{woods75link} and implemented by Brachman~\cite{Brachman91livingwith} in a system called Knowledge Language One (KL-ONE). %The KL-ONE and many versions of description logics are subsets of classical first-order logic (FOL). They belong to the class of monotonic logics, in which new information monotonically increases the number of provable theorems, and none of the old information can ever be deleted or modified. Some versions of description logics support nonmonotonic reasoning, which allows default rules to add optional information and canceling rules to block inherited information. Such systems can be useful for many applications, but they can also create problems of conflicting defaults.
%Although the basic methods of description logics are as old as Aristotle, they remain a vital part of many versions of semantic networks and other kinds of systems.
%Much of the ongoing research on description logics has been devoted to increasing their expressive power while remaining within an efficiently computable subset of logic (Brachman et al. 1991; Woods and Schmolze 1992).
Recent description logics are DAML+OIL~\cite{Horrocks02daml+oil} and its successor OWL~\cite{OWL}, which are intended for representing knowledge in the Semantic Web~\cite{Berners-Lee01}---a giant semantic network that spans the entire Internet.

\here

\subsection{Graph Representation of RDF}
\label{sec:rdfgraph}
According to the W3C specification for the RDF semantics~\cite{Hayes_rdf2004}, an RDF graph, or simply a graph, is defined as a set of RDF triples. A subgraph of an RDF graph is a subset of the triples in the graph. A triple is identified with the singleton set containing it, so that each triple in a graph is considered to be a subgraph. A proper subgraph is a proper subset of the triples in the graph. A ground RDF graph is one with no blank nodes. RDF triples can be visualized as a \emph{directed labeled graph} (see details in Chapter~\ref{chap:representation}). The directed labeled graph model for RDF is straightforward and convenient in most cases. But inconsistency arises when using triples to make assertions on predicates. The directed labeled graph model of RDF makes the artificial distinction between resources and properties. The results of the understanding of RDF bounded by this model becomes especially evident in the limitations of current RDF query languages as studied in~\cite{Angles04rdfquery}.


A hypergraph~\cite{Hypergraph} is a generalization of a traditional graph where edges, called hyperedges, can connect more than two vertices. If each edge in a hypergraph covers the same number of nodes, it is called $r$-uniform hypergraph, $r$ being the number of nodes on each edge.

Hayes has proposed to use hypergraphs to represent RDF~\cite{GraphModelRDF}. In his proposal, any RDF graph can be represented by a simple ordered 3-uniform hypergraph, in which an RDF triple corresponds to a hypergraph edge, the nodes being the subject, predicate and object in this order. In this way, both meta-data and data level statements can be integrated in a consistent model.
This result constitutes an important aspect of the theoretical basis of our proposed graph representation for the combined information source of both data and knowledge.
%bipartite graph-on the one side are hypergraph edges (representing statements, depicted as blank circle nodes); on the other side are hypergraph nodes (representing subject, property, and object; subjects and objects are depicted as solid circles and properties are rectangles). The bipartite edges, labeled as S, P, and O to represent the role, indicate the incidence relationship between hypergraph nodes and edges.

\begin{comment}

\begin{figure}[tbh]
\begin{center}
\includegraphics[width=.4\textwidth]{fig/BG.eps}
\end{center}
\caption{\label{fig:BG} .}
\end{figure}

\end{comment}

\begin{mydef}[\textbf{Hypergraph}]\label{def:hypergraph}
Formally, a hypergraph $G = (V,E)$, is a pair in which $V$ is the vertex set and $E$ is the hyperedge set where each $e \in E$ is a subset of $V$. A weighted hypergraph is a hypergraph that has a positive number $w(e)$,  called the weight of a hyperedge $e$, associated with each hyperedge. We denote a weighted hypergraph by $G = (V,E,w)$. The degree of a vertex $v \in V$, $d(v)$, is defined as $d(v) = \sum_{e\in adj(v)}{w(e)}$, where $adj(v)$ denotes the set of edges that are adjacent to $v$. The degree of a hyperedge $e$, denoted as $\delta(e)$, is the number of vertices in $e$, \ie, $\delta(e)=|e|$. A hyperedge $e$ is said to be incident with a vertex $v$ when $v \in e$. The hypergraph incidence matrix $\mathbf{H} \in \mathbb{R}^{|V| \times |E|}$ is defined as
\begin{equation}
\notag h(v,e)=\left\{\begin{array}{cl}
	   1, & v \in e \\
	   0, & otherwise
	   \end{array}\right.
\end{equation}
Throughout the rest of the dissertation, the diagonal matrix forms for $\delta(e)$, $w(e)$, $d(v)$ are denoted as $\mathbf{D}_e$, $\mathbf{W} \in \mathbb{R}^{|E|}$, and $\mathbf{D}_v \in \mathbb{Z}^{|V|}$, respectively.
\end{mydef}

\section{Graphs in Data Mining}
\subsection{Graph Representation of Relational Structure}
\label{sec:graphForRelationalStructure}
An object set endowed with pairwise relationships can be naturally illustrated as a graph in which vertices represent objects, and any two vertices that have some kind of relationship are joined together by an edge. In the case of frequent itemset mining, a set of objects with the co-occurrence relationship can be represented as directed or undirected graphs.

For illustrating this point of view, let us consider a relational table depicted in Figure~\ref{fig:hg_and_rg}(a). One can construct an undirected graph where the set of vertices is the set of relational attributes (column items) and an edge joins two vertices if the they co-occur in a tuple (as illustrated in Figure~\ref{fig:hg_and_rg}(b)). This graph is called the \emph{Gaifman graph}~\cite{Hodkinson02finiteconformal} of a relational structure. The undirected graph can be further enriched by assigning to each edge a weight equal to the support of the 2-itemset consisting of vertices incident to the edge. Cliques (complete subgraphs) in the Gaifman graph, or \emph{Gaifman cliques} for short, are of particular interest because every tuple (ground atom) in data corresponds to a Gaifman clique. However, ambiguity arises as not all Gaifman cliques have matching tuple in the data. There exists cases where cliques are incidental in the sense that several relational ground atoms play together to induce a clique configuration in the Gaifman graph, but no ground atom covers the entire clique (e.g., the clique of $\{A,B,C,D\}$ in Figure~\ref{fig:hg_and_rg}(b) does not correspond to any tuple in the relational table). Further more, given the Gaifman graph, we lose the information of how nodes are related. For example, if $A, B$ and $C$ are products purchased by a particular customer as indicated by a record in the transactional table, this information is no longer available in the graph.

\begin{figure*}[tbh]
\begin{center}
\includegraphics[width=.8\textwidth]{fig/hg_and_rg1.eps}
\end{center}
\caption[An example of simple graph vs. hypergraph for representing a relational table.]{\label{fig:hg_and_rg} (a) an example transaction table; (b) the Gaifman graph representation of the table; (c) The hypergraph representation of the table}
\end{figure*}

A natural way to remedy the ambiguity is to represent the relational data as a hypergraph (see Section~\ref{sec:rdfgraph} for the definition). An edge in the hypergraph, or hyperedge, can connect more than two vertices. In other words, every hyperedge is an arbitrary nonempty subset of vertices. It is obvious that a simple graph is a special kind of hypergraph. In Chapter~\ref{chap:representation}, we propose to employ hypergraphs to model relational structure. In Chapter~\ref{chap:datamining} and~\ref{chap:miningWithOnto} we describe ways to find semantically associated itemsets using hypergraphs. For example, we can construct a hyperedge for each tuple in the relational table. The relational attributes constitute the universe of vertices in the hypergraph. In this representation, each hyperedge has an exact one-to-one correspondent tuple (see Figure~\ref{fig:hg_and_rg}(c), for example).

\subsection{Graph-based similarity}
Data mining algorithms rely on the notion of similarity between data points to make meaningful inferences. When data is in $\mathbb{R}^d$, the standard similarity measure is the Euclidean distance. When data has an explicit link structure, shortest path distance is commonly used. However, neither of these measures incorporates the intuition that two data points are similar to each other if they are connected by a high density region. This latter concept of similarity measure has been shown in experiments to lead to significant improvement in a number of learning tasks, see, for example,~\cite{BlumAndChawla2001,CorduneanuAndJaakkola2002,BousquetEtal03}.

\vspace{.5cm}
\begin{figure}[tbh]
\begin{center}
\includegraphics[width=.3\textwidth]{fig/pplConn.eps}
\end{center}
\caption[A simple graph of friendship relationship.]{\label{fig:pplConn} A simple graph of friendship relationship.}
\end{figure}

Take the simple graph in Figure~\ref{fig:pplConn}, for example, suppose given a task of friend recommendation based on the information in this graph, the interesting question is whether $C$ or $E$ is a better choice of recommendation to $A$. To answer this question, it is natural to compare the similarity measures $s(A, C)$ and $s(A, E)$. In a rough sense, on can identify in the graph that there are two paths between $A$ and $C$, while only one between $A$ and $E$. It's intuitive to conclude that $A$ and $C$ are more similar, or closer, than $A$ and $E$. This gives us a hint that meaningful similarity measures on graphs should satisfy the following two desired properties:
\begin{enumerate}
\item The more paths connecting two nodes, the closer they are.
\item The shorter the paths, the closer they are.
\end{enumerate}
In other words, the more ``short" connections between two given nodes, the more similar those nodes are.
To this end, in Chapter~\ref{chap:datamining} and~\ref{chap:miningWithOnto}, we propose to employ several quantities that satisfy these properties based on the concept of random walk. In the following example, we quantitatively show the property of random walk commute time distance, which characterizes the expected number of steps to take a round trip between a starting node and a target node.

\begin{figure}[tbh]
\centering
\begin{minipage}[c]{\textwidth}\centering
\includegraphics[width=.4\textwidth]{fig/comm-dist-demo.eps}
\end{minipage}
\hfill
\begin{minipage}[c]{\textwidth}\centering
\begin{tabular}{r|r|r|r|r|r || r|r|r|r|r}
\hline\hline
\multicolumn{6}{c||}{Euclidian Distance}	&	\multicolumn{5}{c}{Commute Distance}		\\
\hline\hline							
Index	&	1	&	2	&	3	&	4	&	5	&	1	&	2	&	3	&	4	&	5	\\
\hline
1	&	0	&	1	&	1.85	&	1.85	&	2.41	&	0	&	12.83	&	19.79	&	19.79	&	20.34	\\
\hline
2	&	1	&	0	&	1	&	1	&	1.41	&	12.83	&	0	&	6.96	&	6.96	&	7.51	\\
\hline
3	&	1.85	&	1	&	0	&	1.41	&	1	&	19.79	&	6.96	&	0	&	7.51	&	6.96	\\
\hline
4	&	1.85	&	1	&	1.41	&	0	&	1	&	19.79	&	6.96	&	7.51	&	0	&	6.96	\\
\hline
5	&	2.41	&	1.41	&	1	&	1	&	0	&	20.34	&	7.51	&	6.96	&	6.96	&	0	\\
\hline\hline
\end{tabular}
\end{minipage}
\caption{\label{fig:cd-demo} A comparison between the Euclidean distance and the commute time distance.}
\end{figure}

Figure~\ref{fig:cd-demo} shows a graph of five nodes with a specific edge configuration (the so-called ``lollipop graph''). The Euclidean distances between each pair of nodes are shown in the left-hand side of the corresponding table above and the respective commute time distances are shown on the right-hand side. It can be seen that node 1 and node 3 are equally close to node 2 in terms of their Euclidean distances. However, node 2 and 3 are considered much closer under commute time distance because they are within a much more densely connected subgraph. This shows that, unlike Euclidean distance and shortest path distance, commute time distance between two nodes captures both the length of paths between them and their local neighborhood densities. We also explore other random walk-based measures including the pseudoinverse of the Laplacian matrix and the stationary probability that are closely related to commute time distance. In the following, we describe random walk on simple graphs, and the extension of random walk to hypergraphs is presented in Chapter~\ref{chap:datamining}. %The details of hypergraph-based similarity measures are described in the subsequent sections.
%These methods are described in detail in the following sections and trade-offs between them are studied.

\subsubsection{Random Walks on simple graphs:}
Given a graph and a starting point we select a neighbor of it at random and move to this neighbor then we select a neighbor of this point at random and move to it etc. The random sequence of points selected this way is a random walk on the graph. In other words, a random walker can jump from vertex to vertex and each vertex therefore represents a state of the Markov chain. The average first-passage time $m(k|i)$~\cite{randomwalks} is the average number of steps needed by a random walker for reaching state $k$ for the first time, when starting from state $i$. The symmetrized quantity $n(i,j)=m(j|i)+m(i|j)$ called the average commute time~\cite{randomwalks}, provides a distance measure between any pair of states. The fact that this quantity is indeed a distance on a graph was proved independently by Klein and Randic~\cite{Klein} and Gobel and Jagers~\cite{Gobel}.

The Laplacian matrix $\mathbf{L}$ of a graph is widely used for finding many properties of the graphs in spectral graph theory. Given node degree matrix $\mathbf{D}$ and graph adjacency matrix $\mathbf{A}$, the Laplacian matrix of the graph is defined as $\mathbf{L}=\mathbf{D}-\mathbf{A}$. The normalized Laplacian is given by $\mathbf{L}_N=\mathbf{I}-\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}$, where $\mathbf{I}$ is the identity matrix. The average commute time $n(i,j)$ can be computed in closed form from the Moore-Penrose pseudoinverse of $\mathbf{L}$~\cite{pseudo}, denoted by $\mathbf{L}^+$.

Various quantities derived from random walk on graph has been used in a number of applications. Fouss et al.~\cite{Fouss06random-walkcomputation} compared twelve scoring algorithms based on graph representation of the database to perform collaborative movie recommendation. Pan et al.~\cite{Pan} developed a similarity measure based on random walk steady state probability to discover correlation between multimedia objects containing data of various modalities. Yen et al.~\cite{Yen05clusteringusing} introduced a new k-means clustering algorithm utilizing the random walk average commute time distance. Zhou et al.~\cite{Zhou:2009:GCB:1687627.1687709} presented a unified framework based on neighborhood random walk to integrate structural and attribute similarities for graph clustering.



\begin{comment}

The field of graph mining has seen a rapid explosion in recent years because of new applications in computational biology, software bug localization, and social and communication networking. This book is designed for studying various applications in the context of managing and mining graphs. Graph mining has been studied by the theoretical community extensively in the context of numerous problems such as graph partitioning, node clustering, matching, and connectivity analysis. However the traditional work in the theoretical community
cannot be directly used in practical applications because of the following reasons:

The definitions of problems such as graph partitioning, matching and dimensionality reduction are too ``clean" to be used with real applications. In real applications, the problem may have different variations such as a disk-resident case, a multi-graph case, or other constraints associated with the graphs. In many cases, problems such as frequent sub-graph mining and dense graph mining may have a variety of different flavors for different scenarios.

The size of the applications in real scenarios are often very large. In such cases, the graphs may not be stored in main memory, but may be available only on disk. A classic example of this is the case of web and social network graphs, which may contain millions of nodes. As a result, it is often necessary to design specialized algorithms which are sensitive to disk access efficiency constraints. In some cases, the entire graph may not be available at one time, but may be available in the form of a continuous stream. This is the case in many applications such as social and telecommunication networks in which edges are received continuously.

It is assumed that the underlying graphs are massive and cannot be held in main memory. This change in assumption has a critical
impact on the algorithms which are required to process such graphs. The problems studied in the book include algorithms for frequent pattern mining, graph matching, indexing, classification, clustering, and dense graph mining. In many cases, the problem of graph management and mining has been studied from the perspective of structured and XML data. Where possible, we have clarified the
connections with the methods and algorithms designed by the XML data management community. We also provide a detailed discussion of the application of graph mining algorithms in a number of recent applications such as graph privacy, web and social networks.

Many of the graph algorithms are sensitive to the application scenario in which they are encountered. Therefore, we will study the usage of many of these techniques in real scenarios such as the web, social networks, and biological data. This provides a better understanding of how the algorithms in the book apply to different scenarios. Thus, the book provides a comprehensive summary both from an algorithmic and applied perspective.
\end{comment}

\section{Integration of Heterogeneous Information Sources}
This section describes various background areas related to the contributions of the matching work based on metaheuristics optimization in Chapter~\ref{chap:annotation} that focuses on resolving heterogeneities in schema/ontologies as well as enabling cross dataset meta-analysis.
\label{sec:annotAndMatching}
\subsection{The Multiobjective Optimization Problem and Pareto-Optimality}
Multi-objective optimization problem (also called multi-criteria, multi-performance or vector optimization) can be defined mathematically as to find the vector $X=[x_1, x_2, \ldots, x_k]^T$ which satisfies the following $m$ inequality constraints and $l$ equality constraints:
\begin{align}
\notag g_i(X)&\geq0, i=1,2,\ldots,m\\
\notag h_i(X)&=0, i=1,2,\ldots,l
\end{align}
and optimize the objective function vector
\begin{align}
\notag F(X)=[f_1(X), f_2(X), \ldots, f_N(X)]^T
\end{align}
where $X=[x_1, x_2, \ldots, x_k]^T$ is called the decision variable vector.

%%
Real-life problems require simultaneous optimization of several incommensurable and often conflicting objectives. Usually, there is no single optimal solution, but there is a set of alternative solutions. These solutions are optimal in the sense that no other solutions in the search space are superior to each other when all the objectives are considered~\cite{SumanSurvey}. They are known as Pareto-optimal solutions. To define the concept of Pareto optimality, we take the example of a minimization problem with two decision vectors $a, b\in X$. Vector $a$ is said to dominate $b$ if
\begin{align}
\notag \forall i &= \{1,2,\ldots,N\}~:~f_i(a)\leq f_i(b)\\
\notag and\\
\notag \exists j &= \{1,2,\ldots,N\}~:~f_j(a)<f_j(b).
\end{align}
When the objectives associated with any pair of non-dominated solutions are compared, it is found that each solution is superior with respect to at least one objective. The set of non-dominated solutions to a multi-objective optimization problem is known as the Pareto-optimal set (Pareto front)~\cite{Zitzler98multiobjectiveoptimization}.

\subsubsection{Metaheuristics on Solving Multi-Objective Optimization Problems}
\label{metaheuristics}
Metaheuristics are used for combinatorial optimization in which an optimal solution is sought over a large, discrete search-space. Popular metaheuristics for combinatorial problems include simulated annealing by Kirkpatrick et al.~\cite{Kirkpatrick1987}, and genetic algorithms by Holland et al.\cite{Holland1992}. Extensive previous research has been devoted to extend these methods to multi-objective optimization problems as discussed in the following, which yield sets of mutually non-dominating solutions that are an approximation to the true Pareto front.

\textbf{Simulated Annealing in Multi-Objective Optimization:} Simulated annealing is based on an analogy of thermodynamics with the way metals cool and anneal. It has been proved to be a compact and robust technique. Simulated annealing was started as a method or tool for solving single objective combinatorial problems, these days it has been applied to solve single as well as multiple objective optimization problems in various fields. A comprehensive survey can be found in~\cite{SumanSurvey}.

\textbf{Evolutionary Multi-Objective Optimization:} Evolutionary multi-objective optimization covers the use of many types of heuristic optimizers inspired by the natural process of evolution. As in nature, a population of individuals (solutions to the problem) exist and, through a process of change and competition between these individuals, the quality of the population is advanced. Deb~\cite{Deb2001} provides an introduction of evolutionary algorithms (e.g., genetic algorithm) for multi-objective as the state of the art.

\subsection{The Schema Matching Problem}
Our study of matching alternative attribute sets is closely related to the schema matching problem in data integration. According to the type of instance value, various instance-based approaches have been developed in previous research. For example, for textual attributes, a linguistic characterization based on information retrieval techniques can be applied~\cite{Rahm01asurvey}; for nominal attributes, evaluation of the degree of overlap of instance values is a preferred approach. Larson et al.~\cite{Larson1989} and Sheth et al.~\cite{Sheth1988} discussed how relationships and entity sets could be integrated primarily based on their domain relationships. Similarity of partially overlapped instance set can be also calculated based on measures such as Hamming distance and Jaccard coefficient; for numeric attributes, most methods use aggregated statistics to characterize the attributes, e.g., `SSN' and `PhoneNo' can be distinguished based on their respective patterns~\cite{Rahm01asurvey}. Hybrid systems that combine several approaches to determine matching often achieve better performance. For example, SemInt~\cite{Li00semint:a} is a comprehensive matching prototype exploiting up to 15 constraint-based and 5 content-based matching criteria. The LSD (Learning Source Descriptions)~\cite{Doan2000} system uses several instance-level matchers (learners) that are trained during a preprocessing step. The iMAP~\cite{Dhamankar04imap} system uses multiple basic matchers, called searches, e.g., text, numeric, category, unit conversion, each of which addresses a particular subset of the match space.


\subsection{The Cluster Matching Problem}
\label{sec:clusterMatching}
In framing our solution to the schema matching problem, in Chapter~\ref{chap:annotation}, we also aim at addressing another challenging task, namely, the problem of finding correspondences among distinct patterns that are observed in different experiments. This is to enable meta-analysis across mining results derived from different sites.

This work is motivated by the problem in our collaborative cross-lab neuroscience ERP (Event Related Potential) pattern analysis~\cite{FrishkoffEtal09, LiuEtal10}. Due to the data-driven strategy we adopt to extract ERP patterns from data, it is natural to formulate the pattern matching problem as the cluster comparison problem. To represent clusterings in a way that meaningful similarity measure can be defined, we choose a clustering representation called density profiles proposed by Bae et al.~\cite{Bae2010} and a clustering similarity index known as ADCO (Attribute Distribution Clustering Orthogonality). The definition of density profile and the ADCO method are briefly described in the following.

\emph{Density Profile}: To represent clusters using density profiles, the attribute's range in each cluster is first discretized into a number of bins, and the similarity between two clusters corresponds to the number of points of each cluster falling within these bins. The formal definition for this number of points is the \textit{density} of an attribute-bin region for cluster $c_k$ in clustering $C$, denoted as $dens_C(k, i, j)$. It refers to the number of points in the region $(i, j)$---the $j$-th bin of the $i$-th attribute---that belongs to the cluster $c_k$ of clustering $C$. For example, for clustering $C$ in Fig.~\ref{fig:density_profile}, $dens_C(1, 1, 1) = 8$, because there are 8 data points in region $(1, 1)$---the first bin of the first attribute $x$---that belongs to the first cluster $c_1$.

%The values of $dens_C(k, i, j)$ for all possible $k$, $i$, $j$ are then listed in a certain ordering to form a clustering's \emph{density profile vector} (defined below). This ordering is imposed on all attribute-bin regions and must be applied to the two datasets in which the clusterings were generated. It is necessary, then, that both datasets must have the same attribute set. If this requirement does not stand, the matching between the sets must be specified in advance. Therefore, in order to apply the density profile method in the ERP pattern matching problem, we must first carry out measure matching. We further discuss the interdependence between pattern matching and metric matching in Section~\ref{sec:discuss}.

The density profile vector $V_C$ for a clustering $C$ is formally defined as an ordered tuple:
\begin{align}
\notag V_C = \bigg[ & dens_C(1, 1, 1), ~\ldots, ~dens_C (1, 1, Q),\\
\notag & dens_C (1, 2, 1), ~\ldots, ~dens_C (1, M, Q),\\
& dens_C (2, 1, 1), ~\ldots, ~dens_C (N, M, Q) \bigg]\, ,\label{eq:densp}
\end{align}
where $Q$ is the number of bins in each of the $M$ attributes, and $N$ is the number of clusters in $C$.

\emph{The ADCO measure}: After the density profile vectors of two clusterings $C$ and $C'$ are obtained, the degree of similarity between  $C$ and $C'$ can be determined by calculating the dot product of the density profile vectors:
$sim(C, C') = V_C  \cdot V_{C'} \, .$

The $ADCO(C,C')$ measure is defined as $sim(C,C')$ normalized by the maximum achievable similarity when using either of the two clusterings:
\begin{equation}
ADCO(C, C') = \frac{sim(C, C')}{NF(C, C')} \, , \label{eq:adco}
\end{equation}
where $NF(C, C') = max \big[sim(C, C), \,sim(C', C')\big]$.

\begin{figure}[tb]
\begin{center}
\includegraphics[width=0.4\textwidth]{fig/density_profile.eps}
\end{center}
\caption[An example of cluster density profiles]{\label{fig:density_profile} Two clusterings $C=\{c_1, c_2\}$ and $C'=\{c_1', c_2'\}$. Two attributes $X$ (attribute 1) and $Y$ (attribute 2) are discretized into 2 bins each. See~\cite{Bae2010} for details.}
\end{figure}

%\subsection{Collective Classification for Schema and Data Matching}
%Collective classification in relational data has become an important and active research topic in the last decade, where class labels for a group of linked instances are correlated and need to be predicted simultaneously~\cite{kong:multi-label}. It has been applied to tackle multiple integration problems that are traditionally solved independently. Wick et al.~\cite{Wick2008} describe a discriminatively-trained model based on Markov random field to perform joint reasoning about schema matching, coreference, and canonicalization. Namata et al.~\cite{Namata2011} proposed an approach consisting of coupled collective classifiers to discover a latent graph structure underlying an observed one by addressing entity resolution, link prediction, and node labeling simultaneously. The difference between these previous methods and our proposed method mainly lies in the fact that we do not require a training phase. Instead, the matchings are discovered by simultaneously optimizing interrelated objective functions which circumvents the labor for acquiring labeled data and the expense of statistical inference.


